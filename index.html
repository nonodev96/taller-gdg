\hypertarget{taller-gdg}{%
\section{Taller GDG}\label{taller-gdg}}

\begin{frame}{Quien soy}
\protect\hypertarget{quien-soy}{}
Soy Antonio Mudarra Machuca investigador en la Universidad de Jaén en el
grupo de investigación SIMIDAT.
\end{frame}

\begin{frame}{Objetivos del taller}
\protect\hypertarget{objetivos-del-taller}{}
Mostrar las capacidades de \textbf{docker} para la ejecución de modelos
de IA, simplificando todo el proceso de configuración de distintos
entornos de desarrollo y ejecución.
\end{frame}

\begin{frame}[fragile]{CUDA y Librerías}
\protect\hypertarget{cuda-y-libreruxedas}{}
Instalación para Windows, accedemos a la web para estudiar como instalar
el kit de desarrollo de CUDA de nvidia
\href{https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/}{CUDA
GUIDE}, podemos descargar los drivers desde
\href{https://developer.nvidia.com/cuda-downloads}{CUDA Toolkit 12.8}

Con \texttt{control\ /name\ Microsoft.DeviceManager} \textgreater{}
Adaptadores de pantalla podemos ver la gráfica que tiene nuestro equípo.
Podemos ver todo el listado de productos de nvidia desde la web
\href{https://developer.nvidia.com/cuda-gpus}{cuda-gpus}.

\begin{block}{Paquetes que incluye}
\protect\hypertarget{paquetes-que-incluye}{}
\begin{itemize}
\tightlist
\item
  CUDA

  \begin{itemize}
  \tightlist
  \item
    CUDA Driver
  \item
    CUDA Runtime (cudart)
  \item
    CUDA Math Library (math.h)
  \end{itemize}
\item
  cuDNN

  \begin{itemize}
  \tightlist
  \item
    CUDA Deep Neural Network
  \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Sistemas operativos compatibles:}
\protect\hypertarget{sistemas-operativos-compatibles}{}
\begin{itemize}
\tightlist
\item
  Microsoft Windows 11 24H2, 22H2-SV2, 23H2
\item
  Microsoft Windows 10 22H2
\item
  Microsoft Windows WSL 2
\item
  Ubuntu 20.04, 22.04, 24.04
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Descarga e instalación}
\protect\hypertarget{descarga-e-instalaciuxf3n}{}
Descarga e instalación para Windows
\href{https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/}{CUDA
Installation Guide for Microsoft Windows}
\end{block}
\end{frame}

\begin{frame}[fragile]
Descarga e instalación para Ubuntu

\href{https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html}{NVIDIA
CUDA Installation Guide for Linux}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Ubuntu 20.04}
\FunctionTok{wget}\NormalTok{ https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86\_64/cuda{-}ubuntu2004.pin}
\FunctionTok{sudo}\NormalTok{ mv cuda{-}ubuntu2004.pin /etc/apt/preferences.d/cuda{-}repository{-}pin{-}600}
\FunctionTok{wget}\NormalTok{ https://developer.download.nvidia.com/compute/cuda/12.8.0/local\_installers/cuda{-}repo{-}ubuntu2004{-}12{-}8{-}local\_12.8.0{-}570.86.10{-}1\_amd64.deb}
\FunctionTok{sudo}\NormalTok{ dpkg {-}i cuda{-}repo{-}ubuntu2004{-}12{-}8{-}local\_12.8.0{-}570.86.10{-}1\_amd64.deb}
\FunctionTok{sudo}\NormalTok{ cp /var/cuda{-}repo{-}ubuntu2004{-}12{-}8{-}local/cuda{-}*{-}keyring.gpg /usr/share/keyrings/}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Ubuntu 22.04}
\FunctionTok{wget}\NormalTok{ https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86\_64/cuda{-}ubuntu2204.pin}
\FunctionTok{sudo}\NormalTok{ mv cuda{-}ubuntu2204.pin /etc/apt/preferences.d/cuda{-}repository{-}pin{-}600}
\FunctionTok{wget}\NormalTok{ https://developer.download.nvidia.com/compute/cuda/12.8.0/local\_installers/cuda{-}repo{-}ubuntu2204{-}12{-}8{-}local\_12.8.0{-}570.86.10{-}1\_amd64.deb}
\FunctionTok{sudo}\NormalTok{ dpkg {-}i cuda{-}repo{-}ubuntu2204{-}12{-}8{-}local\_12.8.0{-}570.86.10{-}1\_amd64.deb}
\FunctionTok{sudo}\NormalTok{ cp /var/cuda{-}repo{-}ubuntu2204{-}12{-}8{-}local/cuda{-}*{-}keyring.gpg /usr/share/keyrings/}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Ubuntu 24.04}
\FunctionTok{wget}\NormalTok{ https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86\_64/cuda{-}ubuntu2404.pin}
\FunctionTok{sudo}\NormalTok{ mv cuda{-}ubuntu2404.pin /etc/apt/preferences.d/cuda{-}repository{-}pin{-}600}
\FunctionTok{wget}\NormalTok{ https://developer.download.nvidia.com/compute/cuda/12.8.0/local\_installers/cuda{-}repo{-}ubuntu2404{-}12{-}8{-}local\_12.8.0{-}570.86.10{-}1\_amd64.deb}
\FunctionTok{sudo}\NormalTok{ dpkg {-}i cuda{-}repo{-}ubuntu2404{-}12{-}8{-}local\_12.8.0{-}570.86.10{-}1\_amd64.deb}
\FunctionTok{sudo}\NormalTok{ cp /var/cuda{-}repo{-}ubuntu2404{-}12{-}8{-}local/cuda{-}*{-}keyring.gpg /usr/share/keyrings/}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# WSL 2}
\FunctionTok{wget}\NormalTok{ https://developer.download.nvidia.com/compute/cuda/repos/wsl{-}ubuntu/x86\_64/cuda{-}wsl{-}ubuntu.pin}
\FunctionTok{sudo}\NormalTok{ mv cuda{-}wsl{-}ubuntu.pin /etc/apt/preferences.d/cuda{-}repository{-}pin{-}600}
\FunctionTok{wget}\NormalTok{ https://developer.download.nvidia.com/compute/cuda/12.8.0/local\_installers/cuda{-}repo{-}wsl{-}ubuntu{-}12{-}8{-}local\_12.8.0{-}1\_amd64.deb}
\FunctionTok{sudo}\NormalTok{ dpkg {-}i cuda{-}repo{-}wsl{-}ubuntu{-}12{-}8{-}local\_12.8.0{-}1\_amd64.deb}
\FunctionTok{sudo}\NormalTok{ cp /var/cuda{-}repo{-}wsl{-}ubuntu{-}12{-}8{-}local/cuda{-}*{-}keyring.gpg /usr/share/keyrings/}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Instalación }
\FunctionTok{sudo}\NormalTok{ apt{-}get update}
\FunctionTok{sudo}\NormalTok{ apt{-}get {-}y install cuda{-}toolkit{-}12{-}8}
\end{Highlighting}
\end{Shaded}

Tras la instalación es posible que se requiera un reinicio, podemos
comprobar si los drivers de nvidia están instalados con el comando
\texttt{nvidia-smi} (NVIDIA System Management Interface).
\end{frame}

\begin{frame}{OLLAMA}
\protect\hypertarget{ollama}{}
Ollama es un gestor de modelos LLM que permite descargar, ejecutar y
desplegar modelo LLM fácilmente mediante una API
\end{frame}

\begin{frame}[fragile]{Docker}
\protect\hypertarget{docker}{}
\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{docker}\NormalTok{ compose build runner}
\ExtensionTok{docker}\NormalTok{ compose up runner {-}d}
\ExtensionTok{docker}\NormalTok{ compose exec runner bash}
\ExtensionTok{docker}\NormalTok{ compose exec runner python {-}c }\StringTok{"import torch; print(torch.cuda.is\_available())"}


\CommentTok{\#x = torch.rand(100, 100, 100, device=\textquotesingle{}cuda:0\textquotesingle{});                                            \textbackslash{}}
\CommentTok{\#del x;                                                                                     \textbackslash{}}
\CommentTok{\#torch.cuda.reset\_max\_memory\_allocated(0);                                                  \textbackslash{}}

\ExtensionTok{docker}\NormalTok{ compose exec runner python {-}c }\KeywordTok{\textbackslash{}}
\StringTok{"import torch;                                                                             \textbackslash{}}
\StringTok{from tabulate import tabulate;                                                             \textbackslash{}}
\StringTok{info\_cuda = [                                                                              \textbackslash{}}
\StringTok{    [\textquotesingle{}torch.\_\_version\_\_\textquotesingle{}, torch.\_\_version\_\_],                                              \textbackslash{}}
\StringTok{    [\textquotesingle{}torch cuda is\_available\textquotesingle{}, torch.cuda.is\_available()],                                \textbackslash{}}
\StringTok{    [\textquotesingle{}torch cuda current\_device\textquotesingle{}, torch.cuda.current\_device()],                            \textbackslash{}}
\StringTok{    [\textquotesingle{}torch cuda device\_count\textquotesingle{}, torch.cuda.device\_count()],                                \textbackslash{}}
\StringTok{    [\textquotesingle{}torch cuda get\_device\_name\textquotesingle{}, torch.cuda.get\_device\_name(0)],                         \textbackslash{}}
\StringTok{    [\textquotesingle{}torch cuda is\_initialized\textquotesingle{}, torch.cuda.is\_initialized()],                            \textbackslash{}}
\StringTok{    [\textquotesingle{}torch cuda memory\_allocated\textquotesingle{}, torch.cuda.memory\_allocated(0)],                       \textbackslash{}}
\StringTok{    [\textquotesingle{}torch cuda memory\_reserved\textquotesingle{}, torch.cuda.memory\_reserved(0)],                         \textbackslash{}}
\StringTok{    [\textquotesingle{}torch cuda max\_memory\_allocated\textquotesingle{}, torch.cuda.max\_memory\_allocated(0)],               \textbackslash{}}
\StringTok{    [\textquotesingle{}torch cuda max\_memory\_reserved\textquotesingle{}, torch.cuda.max\_memory\_reserved(0)],                 \textbackslash{}}
\StringTok{    [\textquotesingle{}torch backends cpu    get\_cpu\_capability\textquotesingle{}, torch.backends.cpu.get\_cpu\_capability()], \textbackslash{}}
\StringTok{    [\textquotesingle{}torch backends cudnn  is\_available\textquotesingle{}, torch.backends.cudnn.is\_available()],           \textbackslash{}}
\StringTok{    [\textquotesingle{}torch backends mkl    is\_available\textquotesingle{}, torch.backends.mkl.is\_available()],             \textbackslash{}}
\StringTok{    [\textquotesingle{}torch backends mkldnn is\_available\textquotesingle{}, torch.backends.mkldnn.is\_available()],          \textbackslash{}}
\StringTok{    [\textquotesingle{}torch backends mps    is\_available\textquotesingle{}, torch.backends.mps.is\_available()],             \textbackslash{}}
\StringTok{    [\textquotesingle{}torch backends openmp is\_available\textquotesingle{}, torch.backends.openmp.is\_available()],          \textbackslash{}}
\StringTok{];                                                                                         \textbackslash{}}
\StringTok{print(tabulate(info\_cuda, headers=[\textquotesingle{}Variable\textquotesingle{}, \textquotesingle{}Value\textquotesingle{}]))                                  \textbackslash{}}
\StringTok{"}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}[fragile]{docker compose de un chat-gpt propio}
\protect\hypertarget{docker-compose-de-un-chat-gpt-propio}{}
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{services}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{open{-}webui}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{image}\KeywordTok{:}\AttributeTok{ ghcr.io/open{-}webui/open{-}webui:main}
\AttributeTok{    }\FunctionTok{container\_name}\KeywordTok{:}\AttributeTok{ $\{PROJECT\_NAME\}\_open{-}webui}
\AttributeTok{    }\FunctionTok{volumes}\KeywordTok{:}
\AttributeTok{      }\KeywordTok{{-}}\AttributeTok{ local{-}open{-}webui:/app/backend/data}
\AttributeTok{    }\FunctionTok{depends\_on}\KeywordTok{:}
\AttributeTok{      }\KeywordTok{{-}}\AttributeTok{ ollama}
\AttributeTok{    }\FunctionTok{ports}\KeywordTok{:}
\AttributeTok{      }\KeywordTok{{-}}\AttributeTok{ $\{OPEN\_WEBUI\_PORT{-}3000\}:8080}
\AttributeTok{    }\FunctionTok{environment}\KeywordTok{:}
\AttributeTok{      }\KeywordTok{{-}}\AttributeTok{ }\StringTok{\textquotesingle{}OLLAMA\_BASE\_URL=http://ollama:11434\textquotesingle{}}
\AttributeTok{    }\FunctionTok{extra\_hosts}\KeywordTok{:}
\AttributeTok{      }\KeywordTok{{-}}\AttributeTok{ host.docker.internal:host{-}gateway}
\AttributeTok{    }\FunctionTok{restart}\KeywordTok{:}\AttributeTok{ unless{-}stopped}

\AttributeTok{  }\FunctionTok{ollama}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{image}\KeywordTok{:}\AttributeTok{ ollama/ollama:latest}
\AttributeTok{    }\FunctionTok{container\_name}\KeywordTok{:}\AttributeTok{ $\{PROJECT\_NAME\}\_ollama}
\AttributeTok{    }\FunctionTok{volumes}\KeywordTok{:}
\AttributeTok{      }\KeywordTok{{-}}\AttributeTok{ local{-}ollama:/root/.ollama}
\AttributeTok{    }\FunctionTok{pull\_policy}\KeywordTok{:}\AttributeTok{ always}
\AttributeTok{    }\FunctionTok{tty}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\AttributeTok{    }\FunctionTok{restart}\KeywordTok{:}\AttributeTok{ unless{-}stopped}
\CommentTok{    \# GPU support}
\AttributeTok{    }\FunctionTok{deploy}\KeywordTok{:}
\AttributeTok{      }\FunctionTok{resources}\KeywordTok{:}
\AttributeTok{        }\FunctionTok{reservations}\KeywordTok{:}
\AttributeTok{          }\FunctionTok{devices}\KeywordTok{:}
\AttributeTok{            }\KeywordTok{{-}}\AttributeTok{ }\FunctionTok{driver}\KeywordTok{:}\AttributeTok{ nvidia}
\AttributeTok{              }\FunctionTok{count}\KeywordTok{:}\AttributeTok{ }\DecValTok{1}
\AttributeTok{              }\FunctionTok{capabilities}\KeywordTok{:}
\AttributeTok{                }\KeywordTok{{-}}\AttributeTok{ gpu}
\FunctionTok{volumes}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{local{-}ollama}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{external}\KeywordTok{:}\AttributeTok{ }\CharTok{false}
\AttributeTok{  }\FunctionTok{local{-}open{-}webui}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{external}\KeywordTok{:}\AttributeTok{ }\CharTok{false}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}[fragile]{Traefik}
\protect\hypertarget{traefik}{}
Para traefik debemos añadir la redireccion al servicio, con
ubuntu/debian \texttt{sudo\ nano\ /etc/hosts} y para Windows abrir
editor de texto con permisos de administrador el fichero
\texttt{C:\textbackslash{}Windows\textbackslash{}System32\textbackslash{}drivers\textbackslash{}etc\textbackslash{}hosts}
y añadir la

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Añadimos el host}
\ExtensionTok{127.0.0.1}\NormalTok{ chat.nonodev96.dev}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}{Referencias}
\protect\hypertarget{referencias}{}
\begin{itemize}
\tightlist
\item
  \href{https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/}{CUDA
  GUIDE}.
\item
  \href{https://developer.nvidia.com/cudnn}{cuDNN}.
\item
  \href{https://docs.openwebui.com/}{OPEN WEB UI}.
\end{itemize}
\end{frame}
